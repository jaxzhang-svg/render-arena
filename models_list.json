{
  "data": [
    {
      "created": 1768823553,
      "id": "zai-org/glm-4.7-flash",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 700,
      "output_token_price_per_m": 4000,
      "title": "zai-org/glm-4.7-flash",
      "description": "GLM-4.7-Flash, a state-of-the-art model in the 30B class, delivers a compelling balance of high performance and efficiency. Tailored for Agentic Coding, it strengthens coding proficiency, long-horizon planning, and tool synergy, securing top-tier results on public benchmarks among similarly sized open-source models. It excels in complex agent tasks with superior instruction following for tool use, while significantly elevating the frontend aesthetics and completion efficiency of long-range workflows in Artifacts and Agentic Coding.",
      "tags": [],
      "context_size": 200000,
      "status": 1,
      "display_name": "GLM-4.7-Flash",
      "model_type": "chat",
      "max_output_tokens": 131100,
      "features": ["serverless", "function-calling", "structured-outputs", "reasoning"],
      "endpoints": ["chat/completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1764588967,
      "id": "deepseek/deepseek-v3.2",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 2690,
      "output_token_price_per_m": 4000,
      "title": "deepseek/deepseek-v3.2",
      "description": "We introduce DeepSeek-V3.2, a next-generation foundation model designed to unify high computational efficiency with state-of-the-art reasoning and agentic performance. DeepSeek-V3.2 is built upon three core technical breakthroughs:\n\n• DeepSeek Sparse Attention (DSA):\nA new highly efficient attention mechanism that significantly reduces computational overhead while preserving model quality, purpose-built for long-context reasoning and high-throughput workloads.\n\n• Scalable Reinforcement Learning Framework:\nDeepSeek-V3.2 leverages a robust RL training protocol and expanded post-training compute to reach GPT-5-level performance. Its high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and demonstrates reasoning capabilities comparable to Gemini-3.0-Pro.\n\n• Large-Scale Agentic Task Synthesis Pipeline:\nTo enable reliable tool-use and multi-step decision-making, we develop a novel agentic data synthesis pipeline that generates high-quality interactive reasoning tasks at scale, greatly enhancing the model’s",
      "tags": [],
      "context_size": 163840,
      "status": 1,
      "display_name": "Deepseek V3.2",
      "model_type": "chat",
      "max_output_tokens": 65536,
      "features": ["function-calling", "structured-outputs", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "completions", "anthropic"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1766485136,
      "id": "minimax/minimax-m2.1",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 3000,
      "output_token_price_per_m": 12000,
      "title": "minimax/minimax-m2.1",
      "description": "MiniMax M2.1 is a cutting-edge AI model designed to revolutionize how developers build software. With enhanced multi-language programming support, it excels in generating high-quality code across popular languages like Rust, Java, Golang, C++, Kotlin, Objective-C, TypeScript, and JavaScript.\n\nKey improvements include:\n\n22% faster response times and 30% lower token consumption for efficient workflows.\nSeamless integration with leading development frameworks (Claude Code, Droid Factory AI, BlackBox, etc.).\nFull-stack development capabilities, from mobile (Android/iOS) to web and 3D interactive prototyping.\nOptimized performance-to-cost ratio, making AI-assisted development more accessible.\nWhether you're a software engineer, app developer, or tech innovator, M2.1 empowers smarter coding with industry-leading AI.",
      "tags": [],
      "context_size": 204800,
      "status": 1,
      "display_name": "Minimax M2.1",
      "model_type": "chat",
      "max_output_tokens": 131072,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions", "anthropic"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1766422299,
      "id": "zai-org/glm-4.7",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 6000,
      "output_token_price_per_m": 22000,
      "title": "zai-org/glm-4.7",
      "description": "GLM-4.7 is Z.AI's latest flagship model, with major upgrades focused on advanced coding capabilities and more reliable multi-step reasoning and execution. It shows clear gains in complex agent workflows, while delivering a more natural conversational experience and stronger front-end design sensibility.",
      "tags": [],
      "context_size": 204800,
      "status": 1,
      "display_name": "GLM-4.7",
      "model_type": "chat",
      "max_output_tokens": 131072,
      "features": ["function-calling", "structured-outputs", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "anthropic", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1766140400,
      "id": "xiaomimimo/mimo-v2-flash",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 1000,
      "output_token_price_per_m": 3000,
      "title": "xiaomimimo/mimo-v2-flash",
      "description": "Xiaomi MiMo-V2-Flash is a proprietary MoE model developed by Xiaomi, designed for extreme inference efficiency with 309B total parameters (15B active). By incorporating an innovative Hybrid attention architecture and multi-layer MTP inference acceleration, it ranks among the top 2 global open-source models across multiple Agent benchmarks. Its coding capabilities surpass all open-source models and rival the industry-leading closed-source model, Claude 4.5 Sonnet—yet at only 2.5% of the inference cost and with 2x the generation speed, successfully pushing the limits of both model performance and efficiency.",
      "tags": [],
      "context_size": 262144,
      "status": 1,
      "display_name": "XiaomiMiMo/MiMo-V2-Flash",
      "model_type": "chat",
      "max_output_tokens": 32000,
      "features": ["serverless", "function-calling", "structured-outputs", "reasoning"],
      "endpoints": ["chat/completions", "completions", "anthropic"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1765370191,
      "id": "zai-org/autoglm-phone-9b-multilingual",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 350,
      "output_token_price_per_m": 1380,
      "title": "zai-org/autoglm-phone-9b-multilingual",
      "description": "Phone Agent is a mobile intelligent assistant framework built on AutoGLM, capable of understanding smartphone screens through multimodal perception and executing automated operations to complete tasks.\nThe system controls devices via ADB (Android Debug Bridge), uses a vision-language model for screen understanding, and leverages intelligent planning to generate and execute action sequences.\n\nUsers can simply describe tasks in natural language—for example, “Open Xiaohongshu and search for food recommendations.”\nPhone Agent will automatically parse the intent, understand the current UI, plan the next steps, and carry out the entire workflow.\n\nThe system also includes:\n\nSensitive action confirmation mechanisms\nHuman-in-the-loop fallback for login or verification code scenarios\nRemote ADB debugging, allowing device connection via WiFi or network for flexible remote control and development",
      "tags": [],
      "context_size": 65536,
      "status": 1,
      "display_name": "AutoGLM-Phone-9B-Multilingual",
      "model_type": "chat",
      "max_output_tokens": 65536,
      "features": ["serverless"],
      "endpoints": ["chat/completions"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1762480786,
      "id": "moonshotai/kimi-k2-thinking",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 6000,
      "output_token_price_per_m": 25000,
      "title": "moonshotai/kimi-k2-thinking",
      "description": "The kimi-k2-thinking model is a general-purpose agentic reasoning model developed by Moonshot AI. ",
      "tags": [],
      "context_size": 262144,
      "status": 1,
      "display_name": "Kimi K2 Thinking",
      "model_type": "chat",
      "max_output_tokens": 262144,
      "features": ["function-calling", "structured-outputs", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "anthropic"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1761536024,
      "id": "minimax/minimax-m2",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 3000,
      "output_token_price_per_m": 12000,
      "title": "minimax/minimax-m2",
      "description": "MiniMax-M2 is a compact, high-efficiency large language model optimized for end-to-end coding and agentic workflows. With 10 billion activated parameters (230 billion total), it delivers near-frontier intelligence across general reasoning, tool use, and multi-step task execution while maintaining low latency and deployment efficiency.\n\nThe model excels in code generation, multi-file editing, compile-run-fix loops, and test-validated repair, showing strong results on SWE-Bench Verified, Multi-SWE-Bench, and Terminal-Bench. It also performs competitively in agentic evaluations such as BrowseComp and GAIA, effectively handling long-horizon planning, retrieval, and recovery from execution errors.\n\nBenchmarked by Artificial Analysis, MiniMax-M2 ranks among the top open-source models for composite intelligence, spanning mathematics, science, and instruction-following. Its small activation footprint enables fast inference, high concurrency, and improved unit economics, making it well-suited for large-scale agents, d",
      "tags": [],
      "context_size": 204800,
      "status": 1,
      "display_name": "MiniMax-M2",
      "model_type": "chat",
      "max_output_tokens": 131072,
      "features": ["function-calling", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "anthropic"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1761138629,
      "id": "paddlepaddle/paddleocr-vl",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 200,
      "output_token_price_per_m": 200,
      "title": "paddlepaddle/paddleocr-vl",
      "description": "",
      "tags": [],
      "context_size": 16384,
      "status": 1,
      "display_name": "PaddleOCR-VL",
      "model_type": "chat",
      "max_output_tokens": 16384,
      "endpoints": ["chat/completions"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1759142233,
      "id": "deepseek/deepseek-v3.2-exp",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 2700,
      "output_token_price_per_m": 4100,
      "title": "deepseek/deepseek-v3.2-exp",
      "description": "DeepSeek-V3.2-Exp is an experimental model introducing the groundbreaking DeepSeek Sparse Attention (DSA) mechanism for enhanced long-context processing efficiency.\n\nBuilt on V3.1-Terminus, DSA achieves fine-grained sparse attention while maintaining identical output quality. This delivers substantial computational efficiency improvements without compromising accuracy.\n\nComprehensive benchmarks confirm V3.2-Exp matches V3.1-Terminus performance, proving efficiency gains don't sacrifice capability. As both a powerful tool and research platform, it establishes new paradigms for efficient long-context AI processing.\n\n",
      "tags": [],
      "context_size": 163840,
      "status": 1,
      "display_name": "Deepseek V3.2 Exp",
      "model_type": "chat",
      "max_output_tokens": 65536,
      "features": ["function-calling", "structured-outputs", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "completions", "anthropic"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1758695177,
      "id": "qwen/qwen3-vl-235b-a22b-thinking",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 9800,
      "output_token_price_per_m": 39500,
      "title": "qwen/qwen3-vl-235b-a22b-thinking",
      "description": "",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "Qwen3 VL 235B A22B Thinking",
      "model_type": "chat",
      "max_output_tokens": 32768,
      "features": ["reasoning", "serverless"],
      "endpoints": ["chat/completions"],
      "input_modalities": ["text", "image", "video"],
      "output_modalities": ["text"]
    },
    {
      "created": 1765204180,
      "id": "zai-org/glm-4.6v",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 3000,
      "output_token_price_per_m": 9000,
      "title": "zai-org/glm-4.6v",
      "description": "GLM-4.6V represents a significant multimodal advancement in the GLM series, featuring a 128k-token training context window and achieving state-of-the-art visual understanding accuracy for models of its parameter scale. Notably, it's the first visual model to natively integrate Function Call capabilities directly into its architecture, creating a seamless pathway from visual perception to executable actions. This breakthrough establishes a unified technical foundation for deploying multimodal agents in real-world business applications.",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "GLM 4.6V",
      "model_type": "chat",
      "max_output_tokens": 32768,
      "features": ["function-calling", "structured-outputs", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "anthropic", "completions"],
      "input_modalities": ["text", "video", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1759220284,
      "id": "zai-org/glm-4.6",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 5500,
      "output_token_price_per_m": 22000,
      "title": "zai-org/glm-4.6",
      "description": "As the latest iteration in the GLM series, GLM-4.6 achieves comprehensive enhancements across multiple domains, including real-world coding, long-context processing, reasoning, searching, writing, and agentic applications. ",
      "tags": [],
      "context_size": 204800,
      "status": 1,
      "display_name": "GLM 4.6",
      "model_type": "chat",
      "max_output_tokens": 131072,
      "features": ["function-calling", "structured-outputs", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "anthropic", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1767593645,
      "id": "kwaipilot/kat-coder-pro",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 3000,
      "output_token_price_per_m": 12000,
      "title": "kwaipilot/kat-coder-pro",
      "description": "KAT-Coder-Pro V1 by KwaiKAT is a non-reasoning model optimized for agentic coding. It delivers strong performance on reasoning-style tasks while requiring significantly fewer output tokens than peer models. With the 1210 release, it achieved a score of 64 on the Artificial Analysis Intelligence Index, placing it in the global Top 10 and ranking first among all non-reasoning models.",
      "tags": [],
      "context_size": 256000,
      "status": 1,
      "display_name": "Kat Coder Pro",
      "model_type": "chat",
      "max_output_tokens": 128000,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1757534155,
      "id": "qwen/qwen3-next-80b-a3b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 1500,
      "output_token_price_per_m": 15000,
      "title": "qwen/qwen3-next-80b-a3b-instruct",
      "description": "Qwen3-Next uses a highly sparse MoE design: 80B total parameters, but only ~3B activated per inference step. Experiments show that, with global load balancing, increasing total expert parameters while keeping activated experts fixed steadily reduces training loss.Compared to Qwen3’s MoE (128 total experts, 8 routed), Qwen3-Next expands to 512 total experts, combining 10 routed experts + 1 shared expert — maximizing resource usage without hurting performance.\nThe Qwen3-Next-80B-A3B-Instruct performs comparably to our flagship model Qwen3-235B-A22B-Instruct-2507, and shows clear advantages in tasks requiring ultra-long context (up to 256K tokens).\n",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "Qwen3 Next 80B A3B Instruct",
      "model_type": "chat",
      "max_output_tokens": 32768,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions", "completions", "anthropic"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1757533943,
      "id": "qwen/qwen3-next-80b-a3b-thinking",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 1500,
      "output_token_price_per_m": 15000,
      "title": "qwen/qwen3-next-80b-a3b-thinking",
      "description": "Qwen3-Next uses a highly sparse MoE design: 80B total parameters, but only ~3B activated per inference step. Experiments show that, with global load balancing, increasing total expert parameters while keeping activated experts fixed steadily reduces training loss.Compared to Qwen3’s MoE (128 total experts, 8 routed), Qwen3-Next expands to 512 total experts, combining 10 routed experts + 1 shared expert — maximizing resource usage without hurting performance.\nThe Qwen3-Next-80B-A3B-Thinking excels at complex reasoning tasks — outperforming higher-cost models like Qwen3-30B-A3B-Thinking-2507 and Qwen3-32B-Thinking, outpeforming the closed-source Gemini-2.5-Flash-Thinking on multiple benchmarks, and approaching the performance of our top-tier model Qwen3-235B-A22B-Thinking-2507.",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "Qwen3 Next 80B A3B Thinking",
      "model_type": "chat",
      "max_output_tokens": 32768,
      "features": ["function-calling", "structured-outputs", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "completions", "anthropic"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1761290363,
      "id": "deepseek/deepseek-ocr",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 300,
      "output_token_price_per_m": 300,
      "title": "deepseek/deepseek-ocr",
      "description": "",
      "tags": [],
      "context_size": 8192,
      "status": 1,
      "display_name": "DeepSeek-OCR",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "endpoints": ["chat/completions"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1758549812,
      "id": "deepseek/deepseek-v3.1-terminus",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 2700,
      "output_token_price_per_m": 10000,
      "title": "deepseek/deepseek-v3.1-terminus",
      "description": "DeepSeek-V3.1-Terminus preserves all original model capabilities while resolving key user-reported issues, including:\n- Language consistency: Significantly reducing mixed Chinese-English output and eliminating abnormal character occurrences\n- Agent performance: Enhanced optimization of both Code Agent and Search Agent functionality",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "Deepseek V3.1 Terminus",
      "model_type": "chat",
      "max_output_tokens": 32768,
      "features": ["function-calling", "structured-outputs", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "completions", "anthropic"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1758695214,
      "id": "qwen/qwen3-vl-235b-a22b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 3000,
      "output_token_price_per_m": 15000,
      "title": "qwen/qwen3-vl-235b-a22b-instruct",
      "description": "",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "Qwen3 VL 235B A22B Instruct",
      "model_type": "chat",
      "max_output_tokens": 32768,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions", "batch-api"],
      "input_modalities": ["text", "image", "video"],
      "output_modalities": ["text"]
    },
    {
      "created": 1758682246,
      "id": "qwen/qwen3-max",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 21100,
      "output_token_price_per_m": 84500,
      "title": "qwen/qwen3-max",
      "description": "Qwen/qwen3-max, Enhanced with specialized upgrades in agent programming and tool calling. This official release achieves domain SOTA performance, supporting more complex agent scenarios.",
      "tags": [],
      "context_size": 262144,
      "status": 1,
      "display_name": "Qwen3 Max",
      "model_type": "chat",
      "max_output_tokens": 65536,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1763437112,
      "id": "skywork/r1v4-lite",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 2000,
      "output_token_price_per_m": 6000,
      "title": "skywork/r1v4-lite",
      "description": "Advanced multimodal large language model supporting both text and image inputs. Features powerful visual understanding and deep reasoning capabilities, suitable for complex task planning and in-depth research scenarios.\n\n",
      "tags": [],
      "context_size": 262144,
      "status": 1,
      "display_name": "Skywork R1V4-Lite",
      "model_type": "chat",
      "max_output_tokens": 65536,
      "features": ["structured-outputs", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1755759094,
      "id": "deepseek/deepseek-v3.1",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 2700,
      "output_token_price_per_m": 10000,
      "title": "deepseek/deepseek-v3.1",
      "description": "DeepSeek-V3.1 is a hybrid model that supports both thinking mode and non-thinking mode.DeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. We have expanded our dataset by collecting additional long documents and substantially extending both training phases. The 32K extension phase has been increased 10-fold to 630B tokens, while the 128K extension phase has been extended by 3.3x to 209B tokens.",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "DeepSeek V3.1",
      "model_type": "chat",
      "max_output_tokens": 32768,
      "features": ["function-calling", "structured-outputs", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "completions", "anthropic"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1757052991,
      "id": "moonshotai/kimi-k2-0905",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 6000,
      "output_token_price_per_m": 25000,
      "title": "moonshotai/kimi-k2-0905",
      "description": "Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It supports long-context inference up to 256k tokens, extended from the previous 128k.\nThis update improves agentic coding with higher accuracy and better generalization across scaffolds, and enhances frontend coding with more aesthetic and functional outputs for web, 3D, and related tasks. Kimi K2 is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. It excels across coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) benchmarks. The model is trained with a novel stack incorporating the MuonClip optimizer for stable large-scale MoE training.",
      "tags": [],
      "context_size": 262144,
      "status": 1,
      "display_name": "Kimi K2 0905",
      "model_type": "chat",
      "max_output_tokens": 262144,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions", "completions", "anthropic"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1753233789,
      "id": "qwen/qwen3-coder-480b-a35b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 3000,
      "output_token_price_per_m": 13000,
      "title": "qwen/qwen3-coder-480b-a35b-instruct",
      "description": "Qwen3-Coder-480B-A35B-Instruct is a cutting-edge open coding model from Qwen, matching Claude Sonnet’s performance in agentic programming, browser automation, and core development tasks. With native 256K context (extendable to 1M tokens via YaRN), it excels at repository-scale analysis and features specialized function-call support for platforms like Qwen Code and CLINE—making it ideal for complex, real-world development workflows.",
      "tags": [],
      "context_size": 262144,
      "status": 1,
      "display_name": "Qwen3 Coder 480B A35B Instruct",
      "model_type": "chat",
      "max_output_tokens": 65536,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions", "completions", "anthropic"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1759991077,
      "id": "qwen/qwen3-coder-30b-a3b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 700,
      "output_token_price_per_m": 2700,
      "title": "qwen/qwen3-coder-30b-a3b-instruct",
      "description": "Qwen3-Coder-30B-A3B-Instruct is a 30.5B parameter Mixture-of-Experts (MoE) model with 128 experts (8 active per forward pass), designed for advanced code generation, repository-scale understanding, and agentic tool use. Built on the Qwen3 architecture, it supports a native context length of 256K tokens (extendable to 1M with Yarn) and performs strongly in tasks involving function calls, browser use, and structured code completion.\n\nThis model is optimized for instruction-following without “thinking mode”, and integrates well with OpenAI-compatible tool-use formats.",
      "tags": [],
      "context_size": 160000,
      "status": 1,
      "display_name": "Qwen3 Coder 30b A3B Instruct",
      "model_type": "chat",
      "max_output_tokens": 32768,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1754438873,
      "id": "openai/gpt-oss-120b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 500,
      "output_token_price_per_m": 2500,
      "title": "openai/gpt-oss-120b",
      "description": "gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation.",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "OpenAI GPT OSS 120B",
      "model_type": "chat",
      "max_output_tokens": 32768,
      "features": ["function-calling", "structured-outputs", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "completions", "batch-api"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1752263515,
      "id": "moonshotai/kimi-k2-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 5700,
      "output_token_price_per_m": 23000,
      "title": "moonshotai/kimi-k2-instruct",
      "description": "Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.Specifically designed for tool use, reasoning, and autonomous problem-solving.",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "Kimi K2 Instruct",
      "model_type": "chat",
      "max_output_tokens": 131072,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions", "anthropic"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1742909352,
      "id": "deepseek/deepseek-v3-0324",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 2700,
      "output_token_price_per_m": 11200,
      "title": "deepseek/deepseek-v3-0324",
      "description": "DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.",
      "tags": [],
      "context_size": 163840,
      "status": 1,
      "display_name": "DeepSeek V3 0324",
      "model_type": "chat",
      "max_output_tokens": 163840,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions", "completions", "anthropic"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1753709673,
      "id": "zai-org/glm-4.5",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 6000,
      "output_token_price_per_m": 22000,
      "title": "zai-org/glm-4.5",
      "description": "GLM-4.5 Series Models are foundation models specifically engineered for intelligent agents. The flagship GLM-4.5 integrates 355 billion total parameters (32 billion active), unifying reasoning, coding, and agent capabilities to address complex application demands.\nAs a hybrid reasoning system, it offers dual operational modes:\n- Thinking Mode: Enables complex reasoning, tool invocation, and strategic planning\n- Non-Thinking Mode: Delivers low-latency responses for real-time interactions\nThis architecture bridges high-performance AI with adaptive functionality for dynamic agent environments.",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "GLM-4.5",
      "model_type": "chat",
      "max_output_tokens": 98304,
      "features": ["function-calling", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1753443150,
      "id": "qwen/qwen3-235b-a22b-thinking-2507",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 3000,
      "output_token_price_per_m": 30000,
      "title": "qwen/qwen3-235b-a22b-thinking-2507",
      "description": "The Qwen3-235B-A22B-Thinking-2507 represents the newest thinking-enabled model in the Qwen3 series, delivering groundbreaking improvements in reasoning capabilities. This advanced AI demonstrates significantly enhanced performance across logical reasoning, mathematics, scientific analysis, coding tasks, and academic benchmarks - matching or even surpassing human-expert level performance to achieve state-of-the-art results among open-source thinking models. Beyond its exceptional reasoning skills, the model shows markedly better general capabilities including more precise instruction following, sophisticated tool usage, highly natural text generation, and improved alignment with human preferences. It also features enhanced 256K long-context understanding, allowing it to maintain coherence and depth across extended documents and complex discussions.",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "Qwen3 235B A22b Thinking 2507",
      "model_type": "chat",
      "max_output_tokens": 32768,
      "features": ["function-calling", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "completions", "anthropic"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1721801867,
      "id": "meta-llama/llama-3.1-8b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 200,
      "output_token_price_per_m": 500,
      "title": "meta-llama/llama-3.1-8b-instruct",
      "description": "Meta's latest class of models, Llama 3.1, launched with a variety of sizes and configurations. The 8B instruct-tuned version is particularly fast and efficient. It has demonstrated strong performance in human evaluations, outperforming several leading closed-source models.",
      "tags": [],
      "context_size": 16384,
      "status": 1,
      "display_name": "Llama 3.1 8B Instruct",
      "model_type": "chat",
      "max_output_tokens": 16384,
      "features": ["serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1756965597,
      "id": "google/gemma-3-12b-it",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 500,
      "output_token_price_per_m": 1000,
      "title": "google/gemma-3-12b-it",
      "description": "",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "Gemma3 12B",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1754914926,
      "id": "zai-org/glm-4.5v",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 6000,
      "output_token_price_per_m": 18000,
      "title": "zai-org/glm-4.5v",
      "description": "Z.ai's GLM-4.5V sets a new standard in visual reasoning, achieving SOTA performance across 42 benchmarks among open-source models. Beyond benchmarks, it excels in real-world applications through hybrid training, enabling comprehensive visual understanding—from image/video analysis and GUI interaction to complex document processing and precise visual grounding.\n\nIn China's GeoGuessr challenge, GLM-4.5V surpassed 99% of 21,000 human players within 16 hours, reaching 66th place in a week. Built on the GLM-4.5-Air foundation and inheriting GLM-4.1V-Thinking's approach, it leverages a 106B-parameter MoE architecture for scalable, efficient performance. This model bridges advanced AI research with practical deployment, delivering unmatched visual intelligence",
      "tags": [],
      "context_size": 65536,
      "status": 1,
      "display_name": "GLM 4.5V",
      "model_type": "chat",
      "max_output_tokens": 16384,
      "features": ["function-calling", "structured-outputs", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text", "video", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1754438961,
      "id": "openai/gpt-oss-20b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 400,
      "output_token_price_per_m": 1500,
      "title": "openai/gpt-oss-20b",
      "description": "gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. It uses a Mixture-of-Experts (MoE) architecture with 3.6B active parameters per forward pass, optimized for lower-latency inference and deployability on consumer or single-GPU hardware. The model is trained in OpenAI’s Harmony response format and supports reasoning level configuration, fine-tuning, and agentic capabilities including function calling, tool use, and structured outputs.",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "OpenAI: GPT OSS 20B",
      "model_type": "chat",
      "max_output_tokens": 32768,
      "features": ["structured-outputs", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1753176794,
      "id": "qwen/qwen3-235b-a22b-instruct-2507",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 900,
      "output_token_price_per_m": 5800,
      "title": "qwen/qwen3-235b-a22b-instruct-2507",
      "description": "Qwen3-235B-A22B-Instruct-2507 is a multilingual, instruction-tuned mixture-of-experts language model based on the Qwen3-235B architecture, with 22B active parameters per forward pass. It is optimized for general-purpose text generation, including instruction following, logical reasoning, math, code, and tool usage. The model supports a native 262K context length and does not implement \"thinking mode\" (\u003cthink\u003e blocks).\nCompared to its base variant, this version delivers significant gains in knowledge coverage, long-context reasoning, coding benchmarks, and alignment with open-ended tasks. It is particularly strong on multilingual understanding, math reasoning (e.g., AIME, HMMT), and alignment evaluations like Arena-Hard and WritingBench.",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "Qwen3 235B A22B Instruct 2507",
      "model_type": "chat",
      "max_output_tokens": 16384,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1738498371,
      "id": "deepseek/deepseek-r1-distill-qwen-14b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 1500,
      "output_token_price_per_m": 1500,
      "title": "deepseek/deepseek-r1-distill-qwen-14b",
      "description": "DeepSeek R1 Distill Qwen 14B is a distilled large language model based on Qwen 2.5 14B, using outputs from DeepSeek R1. It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\nOther benchmark results include:\n\nAIME 2024 pass@1: 69.7\nMATH-500 pass@1: 93.9\nCodeForces Rating: 1481\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
      "tags": [],
      "context_size": 32768,
      "status": 1,
      "display_name": "DeepSeek R1 Distill Qwen 14B",
      "model_type": "chat",
      "max_output_tokens": 16384,
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1733560109,
      "id": "meta-llama/llama-3.3-70b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 1350,
      "output_token_price_per_m": 4000,
      "title": "meta-llama/llama-3.3-70b-instruct",
      "description": "The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\n\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "Llama 3.3 70B Instruct",
      "model_type": "chat",
      "max_output_tokens": 120000,
      "features": ["function-calling", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1728962258,
      "id": "qwen/qwen-2.5-72b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 3800,
      "output_token_price_per_m": 4000,
      "title": "qwen/qwen-2.5-72b-instruct",
      "description": "Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters.",
      "tags": [],
      "context_size": 32000,
      "status": 1,
      "display_name": "Qwen 2.5 72B Instruct",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1722337858,
      "id": "mistralai/mistral-nemo",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 400,
      "output_token_price_per_m": 1700,
      "title": "mistralai/mistral-nemo",
      "description": "A 12B parameter model with a 128k token context length built by Mistral in collaboration with NVIDIA. The model is multilingual, supporting English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi. It supports function calling and is released under the Apache 2.0 license.",
      "tags": [],
      "context_size": 60288,
      "status": 1,
      "display_name": "Mistral Nemo",
      "model_type": "chat",
      "max_output_tokens": 16000,
      "features": ["structured-outputs", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1750139830,
      "id": "minimaxai/minimax-m1-80k",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 5500,
      "output_token_price_per_m": 22000,
      "title": "minimaxai/minimax-m1-80k",
      "description": "MiniMax-M1: The World's First Open-Weight, Large-Scale Hybrid Attention Inference Model\n\nMiniMax-M1 adopts a Mixture of Experts (MoE) architecture and integrates the Flash Attention mechanism. The model contains a total of 456 billion parameters, with 45.9 billion parameters activated per token.\n\nNatively, the M1 model supports a context length of 1 million tokens—8 times that of DeepSeek R1. Additionally, by combining the CISPO algorithm with an efficient hybrid attention design for reinforcement learning training, MiniMax-M1 achieves industry-leading performance in long-context reasoning and real-world software engineering scenarios.",
      "tags": [],
      "context_size": 1000000,
      "status": 1,
      "display_name": "MiniMax M1",
      "model_type": "chat",
      "max_output_tokens": 40000,
      "features": ["function-calling", "reasoning", "serverless"],
      "endpoints": ["chat/completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1748457624,
      "id": "deepseek/deepseek-r1-0528",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 7000,
      "output_token_price_per_m": 25000,
      "title": "deepseek/deepseek-r1-0528",
      "description": "DeepSeek R1 0528 is the latest open-source model released by the DeepSeek team, featuring impressive reasoning capabilities, particularly achieving performance comparable to OpenAI's o1 model in mathematics, coding, and reasoning tasks.",
      "tags": [],
      "context_size": 163840,
      "status": 1,
      "display_name": "DeepSeek R1 0528",
      "model_type": "chat",
      "max_output_tokens": 32768,
      "features": ["function-calling", "structured-outputs", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "completions", "batch-api"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1738498293,
      "id": "deepseek/deepseek-r1-distill-qwen-32b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 3000,
      "output_token_price_per_m": 3000,
      "title": "deepseek/deepseek-r1-distill-qwen-32b",
      "description": "DeepSeek R1 Distill Qwen 32B is a distilled large language model based on Qwen 2.5 32B, using outputs from DeepSeek R1. It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\nOther benchmark results include:\nAIME 2024 pass@1: 72.6\nMATH-500 pass@1: 94.3\nCodeForces Rating: 1691\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
      "tags": [],
      "context_size": 64000,
      "status": 1,
      "display_name": "DeepSeek R1 Distill Qwen 32B",
      "model_type": "chat",
      "max_output_tokens": 32000,
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1714024874,
      "id": "meta-llama/llama-3-8b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 400,
      "output_token_price_per_m": 400,
      "title": "meta-llama/llama-3-8b-instruct",
      "description": "Meta's latest class of model (Llama 3) launched with a variety of sizes \u0026 flavors. This 8B instruct-tuned version was optimized for high quality dialogue usecases. It has demonstrated strong performance compared to leading closed-source models in human evaluations.",
      "tags": [],
      "context_size": 8192,
      "status": 1,
      "display_name": "Llama 3 8B Instruct",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "features": ["serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1713938472,
      "id": "microsoft/wizardlm-2-8x22b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 6200,
      "output_token_price_per_m": 6200,
      "title": "microsoft/wizardlm-2-8x22b",
      "description": "WizardLM-2 8x22B is Microsoft AI's most advanced Wizard model. It demonstrates highly competitive performance compared to leading proprietary models, and it consistently outperforms all existing state-of-the-art opensource models.",
      "tags": [],
      "context_size": 65535,
      "status": 1,
      "display_name": "Wizardlm 2 8x22B",
      "model_type": "chat",
      "max_output_tokens": 8000,
      "features": ["serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1748526709,
      "id": "deepseek/deepseek-r1-0528-qwen3-8b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 600,
      "output_token_price_per_m": 900,
      "title": "deepseek/deepseek-r1-0528-qwen3-8b",
      "description": "DeepSeek-R1-0528-Qwen3-8B is a high-performance reasoning model based on the Qwen3 8B Base model, enhanced through the integration of DeepSeek-R1-0528's Chain-of-Thought (CoT) optimization. In the AIME 2024 evaluation, this open-source model achieved state-of-the-art (SOTA) performance, delivering a 10% improvement over the original Qwen3 8B while matching the reasoning capabilities of the much larger 235-billion-parameter Qwen3-235B-thinking. ",
      "tags": [],
      "context_size": 128000,
      "status": 1,
      "display_name": "DeepSeek R1 0528 Qwen3 8B",
      "model_type": "chat",
      "max_output_tokens": 32000,
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1737957190,
      "id": "deepseek/deepseek-r1-distill-llama-70b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 8000,
      "output_token_price_per_m": 8000,
      "title": "deepseek/deepseek-r1-distill-llama-70b",
      "description": "DeepSeek R1 Distill LLama 70B",
      "tags": [],
      "context_size": 8192,
      "status": 1,
      "display_name": "DeepSeek R1 Distill LLama 70B",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "features": ["structured-outputs", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1714024815,
      "id": "meta-llama/llama-3-70b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 5100,
      "output_token_price_per_m": 7400,
      "title": "meta-llama/llama-3-70b-instruct",
      "description": "Meta's latest class of model (Llama 3) launched with a variety of sizes \u0026 flavors. This 70B instruct-tuned version was optimized for high quality dialogue usecases. It has demonstrated strong performance compared to leading closed-source models in human evaluations.",
      "tags": [],
      "context_size": 8192,
      "status": 1,
      "display_name": "Llama3 70B Instruct",
      "model_type": "chat",
      "max_output_tokens": 8000,
      "features": ["structured-outputs", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1745897024,
      "id": "qwen/qwen3-235b-a22b-fp8",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 2000,
      "output_token_price_per_m": 8000,
      "title": "qwen/qwen3-235b-a22b-fp8",
      "description": "Achieves effective integration of inference and non-inference modes, enabling seamless switching between modes during conversations. The model's inference capability significantly surpasses that of QwQ, and its general capabilities exceed those of Qwen2.5-72B-Instruct, reaching the state-of-the-art (SOTA) level among models of the same scale.",
      "tags": [],
      "context_size": 40960,
      "status": 1,
      "display_name": "Qwen3 235B A22B",
      "model_type": "chat",
      "max_output_tokens": 20000,
      "features": ["reasoning", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1743906990,
      "id": "meta-llama/llama-4-maverick-17b-128e-instruct-fp8",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 2700,
      "output_token_price_per_m": 8500,
      "title": "meta-llama/llama-4-maverick-17b-128e-instruct-fp8",
      "description": "Llama 4 Maverick 17B Instruct (128E) is a high-capacity multimodal language model from Meta, built on a mixture-of-experts (MoE) architecture with 128 experts and 17 billion active parameters per forward pass (400B total). It supports multilingual text and image input, and produces multilingual text and code output across 12 supported languages. Optimized for vision-language tasks, Maverick is instruction-tuned for assistant-like behavior, image reasoning, and general-purpose multimodal interaction.\n\nMaverick features early fusion for native multimodality and a 1 million token context window. It was trained on a curated mixture of public, licensed, and Meta-platform data, covering ~22 trillion tokens, with a knowledge cutoff in August 2024. Released on April 5, 2025 under the Llama 4 Community License, Maverick is suited for research and commercial applications requiring advanced multimodal understanding and high model throughput.",
      "tags": [],
      "context_size": 1048576,
      "status": 1,
      "display_name": "Llama 4 Maverick Instruct",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "features": ["serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1743906925,
      "id": "meta-llama/llama-4-scout-17b-16e-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 1800,
      "output_token_price_per_m": 5900,
      "title": "meta-llama/llama-4-scout-17b-16e-instruct",
      "description": "Llama 4 Scout 17B Instruct (16E) is a mixture-of-experts (MoE) language model developed by Meta, activating 17 billion parameters out of a total of 109B. It supports native multimodal input (text and image) and multilingual output (text and code) across 12 supported languages. Designed for assistant-style interaction and visual reasoning, Scout uses 16 experts per forward pass and features a context length of 10 million tokens, with a training corpus of ~40 trillion tokens.\n\nBuilt for high efficiency and local or commercial deployment, Llama 4 Scout incorporates early fusion for seamless modality integration. It is instruction-tuned for use in multilingual chat, captioning, and image understanding tasks. Released under the Llama 4 Community License, it was last trained on data up to August 2024 and launched publicly on April 5, 2025.",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "Llama 4 Scout Instruct",
      "model_type": "chat",
      "max_output_tokens": 131072,
      "features": ["serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1719493012,
      "id": "nousresearch/hermes-2-pro-llama-3-8b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 1400,
      "output_token_price_per_m": 1400,
      "title": "nousresearch/hermes-2-pro-llama-3-8b",
      "description": "Hermes 2 Pro is an upgraded, retrained version of Nous Hermes 2, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset, as well as a newly introduced Function Calling and JSON Mode dataset developed in-house.",
      "tags": [],
      "context_size": 8192,
      "status": 1,
      "display_name": "Hermes 2 Pro Llama 3 8B",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "features": ["structured-outputs", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1742888969,
      "id": "qwen/qwen2.5-vl-72b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 8000,
      "output_token_price_per_m": 8000,
      "title": "qwen/qwen2.5-vl-72b-instruct",
      "description": "Qwen2.5-VL, the latest vision-language model in the Qwen2.5 series, delivers enhanced multimodal capabilities including advanced visual comprehension for object/text recognition, chart/layout analysis, and agent-based dynamic tool orchestration. It processes long-form videos (\u003e1 hour) with key event detection while enabling precise spatial annotation through bounding boxes or coordinate points. The model specializes in structured data extraction from scanned documents (invoices, tables, etc.) and achieves state-of-the-art performance across multimodal benchmarks encompassing image understanding, temporal video analysis, and agent task evaluations.",
      "tags": [],
      "context_size": 32768,
      "status": 1,
      "display_name": "Qwen2.5 VL 72B Instruct",
      "model_type": "chat",
      "max_output_tokens": 32768,
      "features": ["serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text", "image", "video"],
      "output_modalities": ["text"]
    },
    {
      "created": 1718699128,
      "id": "sao10k/l3-70b-euryale-v2.1",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 14800,
      "output_token_price_per_m": 14800,
      "title": "sao10k/l3-70b-euryale-v2.1",
      "description": "The uncensored llama3 model is a powerhouse of creativity, excelling in both roleplay and story writing. It offers a liberating experience during roleplays, free from any restrictions. This model stands out for its immense creativity, boasting a vast array of unique ideas and plots, truly a treasure trove for those seeking originality. Its unrestricted nature during roleplays allows for the full breadth of imagination to unfold, akin to an enhanced, big-brained version of Stheno. Perfect for creative minds seeking a boundless platform for their imaginative expressions, the uncensored llama3 model is an ideal choice",
      "tags": [],
      "context_size": 8192,
      "status": 1,
      "display_name": "L3 70B Euryale V2.1\t",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "features": ["function-calling", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1758272653,
      "id": "baidu/ernie-4.5-21B-a3b-thinking",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 700,
      "output_token_price_per_m": 2800,
      "title": "baidu/ernie-4.5-21B-a3b-thinking",
      "description": "ERNIE-4.5-21B-A3B-Thinking is a text-based Mixture of Experts (MoE) post-training model featuring 21B total parameters with 3B active parameters per token. It delivers enhanced performance on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise. The model offers efficient tool utilization capabilities and supports up to 128K tokens for long-context understanding.",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "ERNIE-4.5-21B-A3B-Thinking",
      "model_type": "chat",
      "max_output_tokens": 65536,
      "features": ["reasoning", "serverless"],
      "endpoints": ["chat/completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1732791714,
      "id": "sao10k/l3-8b-lunaris",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 500,
      "output_token_price_per_m": 500,
      "title": "sao10k/l3-8b-lunaris",
      "description": "A generalist / roleplaying model merge based on Llama 3.",
      "tags": [],
      "context_size": 8192,
      "status": 1,
      "display_name": "Sao10k L3 8B Lunaris\t",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "features": ["structured-outputs", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1755106514,
      "id": "baichuan/baichuan-m2-32b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 700,
      "output_token_price_per_m": 700,
      "title": "baichuan/baichuan-m2-32b",
      "description": "Baichuan-M2 is a medically-enhanced reasoning model specifically designed for real-world medical reasoning tasks. We begin with real-world medical questions and conduct reinforcement learning training based on a large-scale verifier system. While maintaining the model's general capabilities, the medical effectiveness of Baichuan-M2 has achieved breakthrough improvements.\n\nBaichuan-M2 is currently the world's best open-source medical model. On the HealthBench Benchmark, it surpasses all open-source models, including GPT-OSS-120B, as well as many cutting-edge closed-source models. It is the open-source model closest to GPT-5 in terms of medical capabilities.\n\nOur research demonstrates that a robust verifier is crucial for aligning model capabilities with real-world applications, and an end-to-end reinforcement learning approach fundamentally enhances the model's medical reasoning abilities. The release of Baichuan-M2 represents a significant advancement in the field of medical artificial intelligence, pushing t",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "BaiChuan M2 32B",
      "model_type": "chat",
      "max_output_tokens": 131072,
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1751258620,
      "id": "baidu/ernie-4.5-vl-424b-a47b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 4200,
      "output_token_price_per_m": 12500,
      "title": "baidu/ernie-4.5-vl-424b-a47b",
      "description": "The ERNIE 4.5 series of open-source models adopts a Mixture-of-Experts (MoE) architecture, representing an innovative multimodal heterogeneous model structure. It achieves cross-modal knowledge fusion through a parameter-sharing mechanism while retaining dedicated parameter spaces for individual modalities. This architecture is particularly well-suited for the continuous pre-training paradigm from large language models to multimodal models, significantly enhancing multimodal understanding capabilities while maintaining or even improving performance in text-based tasks. The models are efficiently trained, inferred, and deployed using the PaddlePaddle deep learning framework. During the pre-training of large language models, the Model FLOPs Utilization (MFU) reaches 47%. Experimental results demonstrate that this series of models achieves state-of-the-art (SOTA) performance across multiple text and multimodal benchmarks, with particularly outstanding results in instruction following, world knowledge memorizatio",
      "tags": [],
      "context_size": 123000,
      "status": 1,
      "display_name": "ERNIE 4.5 VL 424B A47B",
      "model_type": "chat",
      "max_output_tokens": 16000,
      "features": ["reasoning", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1751243379,
      "id": "baidu/ernie-4.5-300b-a47b-paddle",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 2800,
      "output_token_price_per_m": 11000,
      "title": "baidu/ernie-4.5-300b-a47b-paddle",
      "description": "The ERNIE 4.5 series of open-source models adopts a Mixture-of-Experts (MoE) architecture, representing an innovative multimodal heterogeneous model structure. It achieves cross-modal knowledge fusion through a parameter-sharing mechanism while retaining dedicated parameter spaces for individual modalities. This architecture is particularly well-suited for the continuous pre-training paradigm from large language models to multimodal models, significantly enhancing multimodal understanding capabilities while maintaining or even improving performance in text-based tasks. The models are efficiently trained, inferred, and deployed using the PaddlePaddle deep learning framework. During the pre-training of large language models, the Model FLOPs Utilization (MFU) reaches 47%. Experimental results demonstrate that this series of models achieves state-of-the-art (SOTA) performance across multiple text and multimodal benchmarks, with particularly outstanding results in instruction following, world knowledge memorizatio",
      "tags": [],
      "context_size": 123000,
      "status": 1,
      "display_name": "ERNIE 4.5 300B A47B",
      "model_type": "chat",
      "max_output_tokens": 12000,
      "features": ["structured-outputs", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1746010994,
      "id": "deepseek/deepseek-prover-v2-671b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 7000,
      "output_token_price_per_m": 25000,
      "title": "deepseek/deepseek-prover-v2-671b",
      "description": "DeepSeek Launches Open-Source Model DeepSeek-Prover-V2-671B, Specializing in Mathematical Theorem Proving\nThe new model employs a Mixture of Experts (MoE) architecture and is trained using the Lean 4 framework for formal reasoning. With 671 billion parameters, it leverages reinforcement learning and large-scale synthetic data to significantly enhance automated theorem-proving capabilities.",
      "tags": [],
      "context_size": 160000,
      "status": 1,
      "display_name": "Deepseek Prover V2 671B",
      "model_type": "chat",
      "max_output_tokens": 160000,
      "features": ["serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1745897287,
      "id": "qwen/qwen3-32b-fp8",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 1000,
      "output_token_price_per_m": 4500,
      "title": "qwen/qwen3-32b-fp8",
      "description": "Achieves effective integration of inference and non-inference modes, allowing seamless switching between modes during conversations. Its inference capability matches that of QwQ-32B with a smaller parameter size, and its general capabilities significantly surpass those of Qwen2.5-14B, reaching the state-of-the-art (SOTA) level among models of the same scale.",
      "tags": [],
      "context_size": 40960,
      "status": 1,
      "display_name": "Qwen3 32B",
      "model_type": "chat",
      "max_output_tokens": 20000,
      "features": ["reasoning", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1745897181,
      "id": "qwen/qwen3-30b-a3b-fp8",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 900,
      "output_token_price_per_m": 4500,
      "title": "qwen/qwen3-30b-a3b-fp8",
      "description": "Achieves effective integration of inference and non-inference modes, allowing seamless switching between modes during conversations. Its inference capability matches that of QwQ-32B with a smaller parameter size, and its general capabilities significantly surpass those of Qwen2.5-14B, reaching the state-of-the-art (SOTA) level among models of the same scale.",
      "tags": [],
      "context_size": 40960,
      "status": 1,
      "display_name": "Qwen3 30B A3B",
      "model_type": "chat",
      "max_output_tokens": 20000,
      "features": ["reasoning", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1742889385,
      "id": "google/gemma-3-27b-it",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 1190,
      "output_token_price_per_m": 2000,
      "title": "google/gemma-3-27b-it",
      "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 32k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs. Gemma 3 27B is Google's latest open source model, successor to Gemma.",
      "tags": [],
      "context_size": 98304,
      "status": 1,
      "display_name": "Gemma 3 27B",
      "model_type": "chat",
      "max_output_tokens": 16384,
      "features": ["serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1741174512,
      "id": "deepseek/deepseek-v3-turbo",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 4000,
      "output_token_price_per_m": 13000,
      "title": "deepseek/deepseek-v3-turbo",
      "description": "DeepSeek-V3 is the latest model from the DeepSeek team, building upon the instruction following and coding abilities of the previous versions. Pre-trained on nearly 15 trillion tokens, the reported evaluations reveal that the model outperforms other open-source models and rivals leading closed-source models.",
      "tags": [],
      "context_size": 64000,
      "status": 1,
      "display_name": "DeepSeek V3 (Turbo)\t",
      "model_type": "chat",
      "max_output_tokens": 16000,
      "features": ["function-calling", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1741174465,
      "id": "deepseek/deepseek-r1-turbo",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 7000,
      "output_token_price_per_m": 25000,
      "title": "deepseek/deepseek-r1-turbo",
      "description": "DeepSeek R1 is the latest open-source model released by the DeepSeek team, featuring impressive reasoning capabilities, particularly achieving performance comparable to OpenAI's o1 model in mathematics, coding, and reasoning tasks.",
      "tags": [],
      "context_size": 64000,
      "status": 1,
      "display_name": "DeepSeek R1 (Turbo)\t",
      "model_type": "chat",
      "max_output_tokens": 16000,
      "features": ["function-calling", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1732875917,
      "id": "Sao10K/L3-8B-Stheno-v3.2",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 500,
      "output_token_price_per_m": 500,
      "title": "Sao10K/L3-8B-Stheno-v3.2",
      "description": "Sao10K/L3-8B-Stheno-v3.2 is a highly skilled actor that excels at fully immersing itself in any role assigned.",
      "tags": [],
      "context_size": 8192,
      "status": 1,
      "display_name": "L3 8B Stheno V3.2",
      "model_type": "chat",
      "max_output_tokens": 32000,
      "features": ["function-calling", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1714024873,
      "id": "gryphe/mythomax-l2-13b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 900,
      "output_token_price_per_m": 900,
      "title": "gryphe/mythomax-l2-13b",
      "description": "The idea behind this merge is that each layer is composed of several tensors, which are in turn responsible for specific functions. Using MythoLogic-L2's robust understanding as its input and Huginn's extensive writing capability as its output seems to have resulted in a model that exceeds at both, confirming my theory. (More details to be released at a later time).",
      "tags": [],
      "context_size": 4096,
      "status": 1,
      "display_name": "Mythomax L2 13B",
      "model_type": "chat",
      "max_output_tokens": 3200,
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1764151133,
      "id": "pa/claude-sonnet-4-5-20250929-dd",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 30000,
      "output_token_price_per_m": 150000,
      "title": "pa/claude-sonnet-4-5-20250929-dd",
      "description": "",
      "tags": [],
      "context_size": 20000,
      "status": 1,
      "display_name": "pa/claude-sonnet-4-5-20250929-dd",
      "model_type": "chat",
      "max_output_tokens": 64000,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1764128673,
      "id": "baidu/ernie-4.5-vl-28b-a3b-thinking",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 3900,
      "output_token_price_per_m": 3900,
      "title": "baidu/ernie-4.5-vl-28b-a3b-thinking",
      "description": "Built upon the powerful ERNIE-4.5-VL-28B-A3B architecture, the newly upgraded ERNIE-4.5-VL-28B-A3B-Thinking achieves a remarkable leap forward in multimodal reasoning capabilities. 🧠✨ Through an extensive mid-training phase, the model absorbed a vast and highly diverse corpus of premium visual-language reasoning data. This massive-scale training process dramatically boosted the model’s representation power while deepening the semantic alignment between visual and language modalities—unlocking unprecedented capabilities in nuanced visual-textual reasoning. 📊\n\nThe model leverages cutting-edge multimodal reinforcement learning techniques on verifiable tasks, integrating GSPO and IcePop strategies to stabilize MoE training combined with dynamic difficulty sampling for exceptional learning efficiency. ⚡ Responding to strong community demand, we’ve significantly strengthened the model’s grounding performance with improved instruction-following capabilities, making visual grounding functions more accessible than eve",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "ERNIE-4.5-VL-28B-A3B-Thinking",
      "model_type": "chat",
      "max_output_tokens": 65536,
      "features": ["function-calling", "structured-outputs", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text", "image", "video"],
      "output_modalities": ["text"]
    },
    {
      "created": 1760687842,
      "id": "qwen/qwen3-vl-8b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 800,
      "output_token_price_per_m": 5000,
      "title": "qwen/qwen3-vl-8b-instruct",
      "description": "",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "qwen/qwen3-vl-8b-instruct",
      "model_type": "chat",
      "max_output_tokens": 32768,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions"],
      "input_modalities": ["text", "image", "video"],
      "output_modalities": ["text"]
    },
    {
      "created": 1760579506,
      "id": "pa/claude-haiku-4-5-20251001",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 10000,
      "output_token_price_per_m": 50000,
      "title": "pa/claude-haiku-4-5-20251001",
      "description": "Our fastest model with near-frontier intelligence",
      "tags": [],
      "context_size": 200000,
      "status": 1,
      "display_name": "Claude Haiku 4.5",
      "model_type": "chat",
      "max_output_tokens": 64000,
      "features": ["function-calling", "structured-outputs", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "anthropic", "completions"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1760367673,
      "id": "zai-org/glm-4.5-air",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 1300,
      "output_token_price_per_m": 8500,
      "title": "zai-org/glm-4.5-air",
      "description": "",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "zai-org/glm-4.5-air",
      "model_type": "chat",
      "max_output_tokens": 98304,
      "features": ["function-calling", "reasoning", "serverless"],
      "endpoints": ["chat/completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1760182393,
      "id": "qwen/qwen3-vl-30b-a3b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 2000,
      "output_token_price_per_m": 7000,
      "title": "qwen/qwen3-vl-30b-a3b-instruct",
      "description": "",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "qwen/qwen3-vl-30b-a3b-instruct",
      "model_type": "chat",
      "max_output_tokens": 32768,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions"],
      "input_modalities": ["text", "video", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1760182346,
      "id": "qwen/qwen3-vl-30b-a3b-thinking",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 2000,
      "output_token_price_per_m": 10000,
      "title": "qwen/qwen3-vl-30b-a3b-thinking",
      "description": "",
      "tags": [],
      "context_size": 131072,
      "status": 1,
      "display_name": "qwen/qwen3-vl-30b-a3b-thinking",
      "model_type": "chat",
      "max_output_tokens": 32768,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions"],
      "input_modalities": ["text", "image", "video"],
      "output_modalities": ["text"]
    },
    {
      "created": 1759193832,
      "id": "pa/claude-sonnet-4-5-20250929",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 30000,
      "output_token_price_per_m": 150000,
      "title": "pa/claude-sonnet-4-5-20250929",
      "description": "Our smartest model. Best for complex agents, coding, and most advanced tasks",
      "tags": [],
      "context_size": 200000,
      "status": 1,
      "display_name": "Claude Sonnet 4.5",
      "model_type": "chat",
      "max_output_tokens": 64000,
      "features": ["function-calling", "structured-outputs", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "anthropic"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1758695093,
      "id": "qwen/qwen3-omni-30b-a3b-thinking",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 0,
      "output_token_price_per_m": 0,
      "title": "qwen/qwen3-omni-30b-a3b-thinking",
      "description": "The Qwen-Omni model accepts combined inputs of text and a single additional modality (image, audio, or video) to generate responses in text or speech. It offers a variety of human-like voices, supports speech output in multiple languages and dialects, and is suitable for applications such as text creation, visual recognition, and voice assistants",
      "tags": [],
      "context_size": 65536,
      "status": 1,
      "display_name": "Qwen3 Omni 30B A3B Thinking",
      "model_type": "chat",
      "max_output_tokens": 16384,
      "features": ["function-calling", "structured-outputs", "reasoning", "serverless"],
      "endpoints": ["chat/completions"],
      "input_modalities": ["text", "audio", "video", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1758695025,
      "id": "qwen/qwen3-omni-30b-a3b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 0,
      "output_token_price_per_m": 0,
      "title": "qwen/qwen3-omni-30b-a3b-instruct",
      "description": "The Qwen-Omni model accepts combined inputs of text and a single additional modality (image, audio, or video) to generate responses in text or speech. It offers a variety of human-like voices, supports speech output in multiple languages and dialects, and is suitable for applications such as text creation, visual recognition, and voice assistants",
      "tags": [],
      "context_size": 65536,
      "status": 1,
      "display_name": "Qwen3 Omni 30B A3B Instruct",
      "model_type": "chat",
      "max_output_tokens": 16384,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions"],
      "input_modalities": ["text", "video", "audio", "image"],
      "output_modalities": ["text", "audio"]
    },
    {
      "created": 1756888269,
      "id": "qwen/qwen-mt-plus",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 2500,
      "output_token_price_per_m": 7500,
      "title": "qwen/qwen-mt-plus",
      "description": "Qwen-MT is a large language model optimized for machine translation, built upon the foundation of the Tongyi Qianwen model. It supports translation across 92 languages — including Chinese, English, Japanese, Korean, French, Spanish, German, Thai, Indonesian, Vietnamese, Arabic, and more — enabling seamless multilingual communication.",
      "tags": [],
      "context_size": 16384,
      "status": 1,
      "display_name": "Qwen MT Plus",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "features": ["serverless"],
      "endpoints": ["chat/completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1754459531,
      "id": "pa/claude-opus-4-1-20250805",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 150000,
      "output_token_price_per_m": 750000,
      "title": "pa/claude-opus-4-1-20250805",
      "description": "Exceptional model for specialized tasks requiring advanced reasoning",
      "tags": [],
      "context_size": 200000,
      "status": 1,
      "display_name": "Claude Opus 4.1",
      "model_type": "chat",
      "max_output_tokens": 32000,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions", "anthropic"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1751258754,
      "id": "baidu/ernie-4.5-vl-28b-a3b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 1400,
      "output_token_price_per_m": 5600,
      "title": "baidu/ernie-4.5-vl-28b-a3b",
      "description": "The ERNIE 4.5 series of open-source models adopts a Mixture-of-Experts (MoE) architecture, representing an innovative multimodal heterogeneous model structure. It achieves cross-modal knowledge fusion through a parameter-sharing mechanism while retaining dedicated parameter spaces for individual modalities. This architecture is particularly well-suited for the continuous pre-training paradigm from large language models to multimodal models, significantly enhancing multimodal understanding capabilities while maintaining or even improving performance in text-based tasks. The models are efficiently trained, inferred, and deployed using the PaddlePaddle deep learning framework. During the pre-training of large language models, the Model FLOPs Utilization (MFU) reaches 47%. Experimental results demonstrate that this series of models achieves state-of-the-art (SOTA) performance across multiple text and multimodal benchmarks, with particularly outstanding results in instruction following, world knowledge memorizatio",
      "tags": [],
      "context_size": 30000,
      "status": 1,
      "display_name": "ERNIE 4.5 VL 28B A3B",
      "model_type": "chat",
      "max_output_tokens": 8000,
      "features": ["function-calling", "reasoning", "serverless"],
      "endpoints": ["chat/completions"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1751258682,
      "id": "baidu/ernie-4.5-21B-a3b",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 700,
      "output_token_price_per_m": 2800,
      "title": "baidu/ernie-4.5-21B-a3b",
      "description": "The ERNIE 4.5 series of open-source models adopts a Mixture-of-Experts (MoE) architecture, representing an innovative multimodal heterogeneous model structure. It achieves cross-modal knowledge fusion through a parameter-sharing mechanism while retaining dedicated parameter spaces for individual modalities. This architecture is particularly well-suited for the continuous pre-training paradigm from large language models to multimodal models, significantly enhancing multimodal understanding capabilities while maintaining or even improving performance in text-based tasks. The models are efficiently trained, inferred, and deployed using the PaddlePaddle deep learning framework. During the pre-training of large language models, the Model FLOPs Utilization (MFU) reaches 47%. Experimental results demonstrate that this series of models achieves state-of-the-art (SOTA) performance across multiple text and multimodal benchmarks, with particularly outstanding results in instruction following, world knowledge memorizatio",
      "tags": [],
      "context_size": 120000,
      "status": 1,
      "display_name": "ERNIE 4.5 21B A3B",
      "model_type": "chat",
      "max_output_tokens": 8000,
      "features": ["function-calling", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1747982022,
      "id": "pa/cd-st-4-20250514",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 30000,
      "output_token_price_per_m": 150000,
      "title": "pa/cd-st-4-20250514",
      "description": "",
      "tags": [],
      "context_size": 200000,
      "status": 1,
      "display_name": "Claude Sonnet 4",
      "model_type": "chat",
      "max_output_tokens": 64000,
      "features": ["function-calling", "structured-outputs", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "anthropic"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1747981860,
      "id": "pa/cd-op-4-20250514",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 150000,
      "output_token_price_per_m": 750000,
      "title": "pa/cd-op-4-20250514",
      "description": "",
      "tags": [],
      "context_size": 200000,
      "status": 1,
      "display_name": "Claude Opus 4",
      "model_type": "chat",
      "max_output_tokens": 32000,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions", "anthropic"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1747644168,
      "id": "pa/cd-3-hk-20240307",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 2500,
      "output_token_price_per_m": 12500,
      "title": "pa/cd-3-hk-20240307",
      "description": "claude-3-haiku-20240307",
      "tags": [],
      "context_size": 200000,
      "status": 1,
      "display_name": "Claude Haiku 3",
      "model_type": "chat",
      "max_output_tokens": 4096,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions", "anthropic"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1747644033,
      "id": "pa/cd-3-st-20240229",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 30000,
      "output_token_price_per_m": 150000,
      "title": "pa/cd-3-st-20240229",
      "description": "claude-3-sonnet-20240229",
      "tags": [],
      "context_size": 200000,
      "status": 4,
      "display_name": "Claude Sonnet 3",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1747643866,
      "id": "pa/cd-3-5-hk-20241022",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 8000,
      "output_token_price_per_m": 40000,
      "title": "pa/cd-3-5-hk-20241022",
      "description": "claude-3-5-haiku-20241022",
      "tags": [],
      "context_size": 200000,
      "status": 1,
      "display_name": "Claude Haiku 3.5",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions", "anthropic"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1747642208,
      "id": "pa/cd-3-5-st-20241022",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 30000,
      "output_token_price_per_m": 150000,
      "title": "pa/cd-3-5-st-20241022",
      "description": "claude-3-5-sonnet-20241022",
      "tags": [],
      "context_size": 200000,
      "status": 1,
      "display_name": "Claude Sonnet 3.5",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions", "anthropic"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1745901633,
      "id": "qwen/qwen3-8b-fp8",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 350,
      "output_token_price_per_m": 1380,
      "title": "qwen/qwen3-8b-fp8",
      "description": "Achieves effective integration of reasoning and non-reasoning modes, allowing seamless mode switching during conversations. Its reasoning capability reaches state-of-the-art (SOTA) performance among models of the same scale, and its general capabilities significantly outperform those of Qwen2.5-7B.",
      "tags": [],
      "context_size": 128000,
      "status": 1,
      "display_name": "Qwen3 8B",
      "model_type": "chat",
      "max_output_tokens": 20000,
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1745901504,
      "id": "qwen/qwen3-4b-fp8",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 300,
      "output_token_price_per_m": 300,
      "title": "qwen/qwen3-4b-fp8",
      "description": "Achieves effective integration of reasoning and non-reasoning modes, allowing seamless switching during conversations. The model delivers state-of-the-art (SOTA) reasoning performance among models of the same scale, with significantly enhanced human preference alignment. Notable improvements are seen in creative writing, role-playing, multi-turn dialogue, and instruction following, leading to a clearly improved user experience.",
      "tags": [],
      "context_size": 128000,
      "status": 1,
      "display_name": "Qwen3 4B",
      "model_type": "chat",
      "max_output_tokens": 20000,
      "features": ["reasoning", "serverless"],
      "endpoints": ["chat/completions", "completions", "batch-api"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1745379841,
      "id": "pa/cd-3-7-st-20250219-novita",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 30000,
      "output_token_price_per_m": 150000,
      "title": "pa/cd-3-7-st-20250219-novita",
      "description": "claude-3-7-sonnet-20250219",
      "tags": [],
      "context_size": 200000,
      "status": 1,
      "display_name": "pa/cd-3-7-st-20250219-novita",
      "model_type": "chat",
      "max_output_tokens": 64000,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1745201335,
      "id": "pa/cd-3-7-st-20250219",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 30000,
      "output_token_price_per_m": 150000,
      "title": "pa/cd-3-7-st-20250219",
      "description": "claude-3-7-sonnet-20250219",
      "tags": [],
      "context_size": 200000,
      "status": 1,
      "display_name": "Claude Sonnet 3.7",
      "model_type": "chat",
      "max_output_tokens": 64000,
      "features": ["function-calling", "structured-outputs", "reasoning", "serverless"],
      "endpoints": ["chat/completions", "anthropic"],
      "input_modalities": ["text", "image"],
      "output_modalities": ["text"]
    },
    {
      "created": 1744797581,
      "id": "qwen/qwen2.5-7b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 700,
      "output_token_price_per_m": 700,
      "title": "qwen/qwen2.5-7b-instruct",
      "description": "Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.",
      "tags": [],
      "context_size": 32000,
      "status": 1,
      "display_name": "Qwen2.5 7B Instruct",
      "model_type": "chat",
      "max_output_tokens": 32000,
      "features": ["function-calling", "structured-outputs", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1732607748,
      "id": "meta-llama/llama-3.2-3b-instruct",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 300,
      "output_token_price_per_m": 500,
      "title": "meta-llama/llama-3.2-3b-instruct",
      "description": "The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out)",
      "tags": [],
      "context_size": 32768,
      "status": 1,
      "display_name": "Llama 3.2 3B Instruct",
      "model_type": "chat",
      "max_output_tokens": 32000,
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    },
    {
      "created": 1726742141,
      "id": "sao10k/l31-70b-euryale-v2.2",
      "object": "model",
      "owned_by": "unknown",
      "permission": null,
      "root": "",
      "parent": "",
      "input_token_price_per_m": 14800,
      "output_token_price_per_m": 14800,
      "title": "sao10k/l31-70b-euryale-v2.2",
      "description": "Euryale L3.1 70B v2.2 is a model focused on creative roleplay from Sao10k. It is the successor of Euryale L3 70B v2.1.",
      "tags": [],
      "context_size": 8192,
      "status": 1,
      "display_name": "L31 70B Euryale V2.2",
      "model_type": "chat",
      "max_output_tokens": 8192,
      "features": ["function-calling", "serverless"],
      "endpoints": ["chat/completions", "completions"],
      "input_modalities": ["text"],
      "output_modalities": ["text"]
    }
  ]
}
